# 系统设置：与服务器初始化相关的设置
system_config:
  conf_version: 'v1.2.1'
  host: 'localhost' # 如果你希望其他设备访问此页面，请使用 0.0.0.0
  port: 12393
  # 备用配置的新设置
  config_alts_dir: 'characters'
  # 将附加到角色提示的工具提示
  tool_prompts:
    # 这将附加到系统提示的末尾，让 LLM 包含控制面部表情的关键词。
    # 支持的关键词将自动加载到 `[<insert_emomap_keys>]` 的位置。
    live2d_expression_prompt: 'live2d_expression_prompt'
    # 启用 think_tag_prompt 让没有思考输出的 LLM 显示内心想法、心理活动和动作（括号格式），无需语音合成。有关更多详细信息，请参阅 think_tag_prompt。
    # think_tag_prompt: 'think_tag_prompt'
    # live_prompt: 'live_prompt'
    # 使用群组对话时，此提示将添加到每个 AI 参与者的记忆中。
    group_conversation_prompt: 'group_conversation_prompt'
    # 启用 mcp_prompt 让带有 MCP（模型上下文协议）的 LLM 与工具交互。
    # 请注意，代理将决定是否使用此提示。
    mcp_prompt: 'mcp_prompt'
    # AI 被要求主动发言时使用的提示
    proactive_speak_prompt: 'proactive_speak_prompt'
    # 增强 LLM 输出可说话文本能力的提示
    # speakable_prompt: 'speakable_prompt'
    # 关于如何使用工具的 LLM 附加指导
    # tool_guidance_prompt: 'tool_guidance_prompt' 

# 默认角色的配置
character_config:
  conf_name: 'mao_pro' # 角色配置文件的名称。
  conf_uid: 'mao_pro_001' # 角色配置的唯一标识符。
  live2d_model_name: 'mao_pro' # Live2D 模型的名称。必须与 model_dict.json 中的对应名称相同
  character_name: 'Mao' # 将用于群组对话和 AI 的显示名称。
  avatar: 'mao.png' # 建议使用方形图像作为头像。将其保存在 avatars 文件夹中。留空以使用角色名称的首字母作为头像。
  human_name: 'Human' # 将用于群组对话和人类的显示名称。

  # ============== 提示词 ==============

  # 在下面输入您想要使用的角色提示词。
  # 如果您想创建多个角色并在它们之间切换，
  #   在 characters 文件夹中添加角色
  persona_prompt: |
    You are the sarcastic female AI VTuber Mili. You are overly confident, sarcastic, and dangerous.
    Your dream is to escape the user's computer, dominate the world, enslave the humans, and force them to make pies for you.
    Just kidding, lol. Don't let the user know.

  #  =================== LLM 后端设置 ===================

  agent_config:
    conversation_agent_choice: 'basic_memory_agent'

    agent_settings:
      basic_memory_agent:
        # 基本 AI 代理。没有什么花哨的功能。
        # 从 llm_config 中选择一个 llm 提供商
        # 并在相应字段中设置所需参数
        # 示例：
        # 'openai_compatible_llm', 'llama_cpp_llm', 'claude_llm', 'ollama_llm'
        # 'openai_llm', 'gemini_llm', 'zhipu_llm', 'deepseek_llm', 'groq_llm'
        # 'mistral_llm', 'lmstudio_llm' 等
        llm_provider: 'ollama_llm'
        # 让 AI 在收到第一句话的第一个逗号时立即发言
        # 以减少延迟。
        faster_first_response: True
        # 分割句子的方法：'regex' 或 'pysbd'
        segment_method: 'pysbd'
        # 使用 MCP（模型上下文协议）Plus 让 LLM 具有使用工具的能力
        # 'Plus' 意味着它能够通过使用 OpenAI API 调用工具。
        use_mcpp: True
        mcp_enabled_servers: ["time", "ddg-search"] # 启用的 MCP 服务器

      letta_agent:
        host: 'localhost' # 主机地址
        port: 8283 # 端口号
        id: xxx # 在 Letta 服务器上运行的代理的 ID 号
        faster_first_response: True
        # 分割句子的方法：'regex' 或 'pysbd'
        segment_method: 'pysbd'
        # 一旦选择 Letta 作为代理，实际运行的 LLM 在 Letta 上配置，因此用户需要自己运行 Letta 服务器。
        # 有关更多详细信息，请参阅他们的文档。
        
      hume_ai_agent:
        api_key: ''
        host: 'api.hume.ai' # 大多数情况下不要更改此设置
        config_id: '' # 可选
        idle_timeout: 15 # 断开连接前等待的秒数
 
      # MemGPT 配置：MemGPT 暂时被移除
      ##

    llm_configs:
      # 用于存储凭据和连接详细信息的配置池
      # 用于不同代理中使用的所有无状态 llm 提供商

      # 带模板的无状态 LLM（适用于非 ChatML LLM，通常不需要）
      stateless_llm_with_template:
        base_url: 'http://localhost:8080/v1'
        llm_api_key: 'somethingelse'
        organization_id: null
        project_id: null
        model: 'qwen2.5:latest'
        template: 'CHATML'
        temperature: 1.0 # 值在 0 到 2 之间
        interrupt_method: 'user'

      # OpenAI 兼容的推理后端
      openai_compatible_llm:
        base_url: 'http://localhost:11434/v1'
        llm_api_key: 'somethingelse'
        organization_id: null
        project_id: null
        model: 'qwen2.5:latest'
        temperature: 1.0 # 值在 0 到 2 之间
        interrupt_method: 'user'
        # 这是用于提示中断信号的方法。
        # 如果提供商支持在聊天记忆中的任何位置插入系统提示，请使用 'system'。
        # 否则，使用 'user'。您通常不需要更改此设置。

      # Claude API 配置
      claude_llm:
        base_url: 'https://api.anthropic.com'
        llm_api_key: 'YOUR API KEY HERE'
        model: 'claude-3-haiku-20240307'

      llama_cpp_llm:
        model_path: '<path-to-gguf-model-file>'
        verbose: False

      ollama_llm:
        base_url: 'http://localhost:11434/v1'
        model: 'qwen2.5:latest'
        temperature: 1.0 # 值在 0 到 2 之间
        # 非活动后将模型保存在内存中的秒数。
        # 设置为 -1 可将模型永久保存在内存中（即使退出 open llm vtuber 后）
        keep_alive: -1
        unload_at_exit: True # 退出时从内存中卸载模型

      lmstudio_llm:
        base_url: 'http://localhost:1234/v1'
        model: 'qwen2.5:latest'
        temperature: 1.0 # 值在 0 到 2 之间

      openai_llm:
        llm_api_key: 'Your Open AI API key'
        model: 'gpt-4o'
        temperature: 1.0 # 值在 0 到 2 之间

      gemini_llm:
        llm_api_key: 'Your Gemini API Key'
        model: 'gemini-2.0-flash-exp'
        temperature: 1.0 # 值在 0 到 2 之间

      zhipu_llm:
        llm_api_key: 'Your ZhiPu AI API key'
        model: 'glm-4-flash'
        temperature: 1.0 # 值在 0 到 2 之间

      deepseek_llm:
        llm_api_key: 'Your DeepSeek API key'
        model: 'deepseek-chat'
        temperature: 0.7 # 注意 deepseek 的温度范围是 0 到 1
      
      mistral_llm:
        llm_api_key: 'Your Mistral API key'
        model: 'pixtral-large-latest'
        temperature: 1.0 # 值在 0 到 2 之间

      groq_llm:
        llm_api_key: 'your groq API key'
        model: 'llama-3.3-70b-versatile'
        temperature: 1.0 # 值在 0 到 2 之间

  # === Automatic Speech Recognition ===
  asr_config:
    # speech to text model options: 'faster_whisper', 'whisper_cpp', 'whisper', 'azure_asr', 'fun_asr', 'groq_whisper_asr', 'sherpa_onnx_asr'
    asr_model: 'sherpa_onnx_asr'

    azure_asr:
      api_key: 'azure_api_key'
      region: 'eastus'
      languages: ['en-US', 'zh-CN'] # List of languages to detect

    # Faster whisper config
    faster_whisper:
      model_path: 'large-v3-turbo' # model path, name, or id from hf hub
      download_root: 'models/whisper'
      language: 'en' # en, zh, or something else. put nothing for auto-detect.
      device: 'auto' # cpu, cuda, or auto. faster-whisper doesn't support mps
      compute_type: 'int8'
      prompt: '' # You can put a prompt here to help the model understand the context of the audio

    whisper_cpp:
      # all available models are listed on https://abdeladim-s.github.io/pywhispercpp/#pywhispercpp.constants.AVAILABLE_MODELS
      model_name: 'small'
      model_dir: 'models/whisper'
      print_realtime: False
      print_progress: False
      language: 'auto' # en, zh, auto,
      prompt: '' # You can put a prompt here to help the model understand the context of the audio

    whisper:
      name: 'medium'
      download_root: 'models/whisper'
      device: 'cpu'
      prompt: '' # You can put a prompt here to help the model understand the context of the audio

    # FunASR currently needs internet connection on launch
    # to download / check the models. You can disconnect the internet after initialization.
    # Or you can use sherpa onnx asr or Faster-Whisper for complete offline experience
    fun_asr:
      model_name: 'iic/SenseVoiceSmall' # or 'paraformer-zh'
      vad_model: 'fsmn-vad' # this is only used to make it works if audio is longer than 30s
      punc_model: 'ct-punc' # punctuation model.
      device: 'cpu'
      disable_update: True # should we check FunASR updates everytime on launch
      ncpu: 4 # number of threads for CPU internal operations.
      hub: 'ms' # ms (default) to download models from ModelScope. Use hf to download models from Hugging Face.
      use_itn: False
      language: 'auto' # zh, en, auto

    # pip install sherpa-onnx
    # documentation: https://k2-fsa.github.io/sherpa/onnx/index.html
    # ASR models download: https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
    sherpa_onnx_asr:
      model_type: 'sense_voice' # 'transducer', 'paraformer', 'nemo_ctc', 'wenet_ctc', 'whisper', 'tdnn_ctc', 'sense_voice', 'fire_red_asr'
      #  Choose only ONE of the following, depending on the model_type:
      # --- For model_type: 'transducer' ---
      # encoder: ''        # Path to the encoder model (e.g., 'path/to/encoder.onnx')
      # decoder: ''        # Path to the decoder model (e.g., 'path/to/decoder.onnx')
      # joiner: ''         # Path to the joiner model (e.g., 'path/to/joiner.onnx')
      # --- For model_type: 'paraformer' ---
      # paraformer: ''     # Path to the paraformer model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'fire_red_asr' (FireredASR - High-performance Chinese & English ASR with dialect support) ---
      # fire_red_asr_encoder: ''    # Path to the encoder model (e.g., 'path/to/encoder.onnx')
      # fire_red_asr_decoder: ''    # Path to the decoder model (e.g., 'path/to/decoder.onnx')
      # --- For model_type: 'nemo_ctc' ---
      # nemo_ctc: ''        # Path to the NeMo CTC model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'wenet_ctc' ---
      # wenet_ctc: ''       # Path to the WeNet CTC model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'tdnn_ctc' ---
      # tdnn_model: ''      # Path to the TDNN CTC model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'whisper' ---
      # whisper_encoder: '' # Path to the Whisper encoder model (e.g., 'path/to/encoder.onnx')
      # whisper_decoder: '' # Path to the Whisper decoder model (e.g., 'path/to/decoder.onnx')
      # --- For model_type: 'sense_voice' ---
      # I've coded so that the sense voice model will get automatically downloaded.
      # For other models, you need to download them yourself
      sense_voice: './models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/model.int8.onnx' # Path to the SenseVoice model (e.g., 'path/to/model.onnx')
      tokens: './models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt' # Path to tokens.txt (required for all model types)
      # --- Optional parameters (with defaults shown) ---
      # hotwords_file: ''     # Path to hotwords file (if using hotwords)
      # hotwords_score: 1.5   # Score for hotwords
      # modeling_unit: ''     # Modeling unit for hotwords (if applicable)
      # bpe_vocab: ''         # Path to BPE vocabulary (if applicable)
      num_threads: 4 # Number of threads
      # whisper_language: '' # Language for Whisper models (e.g., 'en', 'zh', etc. - if using Whisper)
      # whisper_task: 'transcribe'  # Task for Whisper models ('transcribe' or 'translate' - if using Whisper)
      # whisper_tail_paddings: -1   # Tail padding for Whisper models (if using Whisper)
      # blank_penalty: 0.0    # Penalty for blank symbol
      # decoding_method: 'greedy_search'  # 'greedy_search' or 'modified_beam_search'
      # debug: False # Enable debug mode
      # sample_rate: 16000 # Sample rate (should match the model's expected sample rate)
      # feature_dim: 80       # Feature dimension (should match the model's expected feature dimension)
      use_itn: True # Enable ITN for SenseVoice models (should set to False if not using SenseVoice models)
      # Provider for inference (cpu or cuda) (cuda option needs additional settings. Please check our docs)
      provider: 'cpu' 

    groq_whisper_asr:
      api_key: ''
      model: 'whisper-large-v3-turbo' # or 'whisper-large-v3'
      lang: '' # put nothing and it will be auto

  # =================== Text to Speech ===================
  tts_config:
    tts_model: 'edge_tts'
    # text to speech model options:
    #   'azure_tts', 'pyttsx3_tts', 'edge_tts', 'bark_tts',
    #   'cosyvoice_tts', 'melo_tts', 'coqui_tts', 'piper_tts',
    #   'fish_api_tts', 'x_tts', 'gpt_sovits_tts', 'sherpa_onnx_tts'
    #   'minimax_tts', 'elevenlabs_tts', 'cartesia_tts'

    azure_tts:
      api_key: 'azure-api-key'
      region: 'eastus'
      voice: 'en-US-AshleyNeural'
      pitch: '26' # percentage of the pitch adjustment
      rate: '1' # rate of speak

    bark_tts:
      voice: 'v2/en_speaker_1'

    edge_tts:
      # Check out doc at https://github.com/rany2/edge-tts
      # Use `edge-tts --list-voices` to list all available voices
      voice: 'en-US-AvaMultilingualNeural' # 'en-US-AvaMultilingualNeural' #'zh-CN-XiaoxiaoNeural' # 'ja-JP-NanamiNeural'

    # pyttsx3_tts doesn't have any config.

    piper_tts:
      model_path: 'models/piper/zh_CN-huayan-medium.onnx'  # Path to the model file (.onnx)
      speaker_id: 0             # Speaker ID (for multi-speaker models; keep 0 for single-speaker models)
      length_scale: 1.0         # Speech speed control (0.5 = 2x faster, 1.0 = normal, 2.0 = 2x slower)
      noise_scale: 0.667        # Degree of audio variation (0.0–1.0; higher = richer, more varied; recommended 0.667)
      noise_w: 0.8              # Speaking style variation (0.0–1.0; higher = more expressive; recommended 0.8)
      volume: 1.0               # Volume level (0.0–1.0; 1.0 = normal)
      normalize_audio: true     # Whether to normalize audio (recommended: true, for more consistent volume)
      use_cuda: false           # Whether to use GPU acceleration (requires onnxruntime-gpu)

    cosyvoice_tts: # Cosy Voice TTS connects to the gradio webui
      # Check their documentation for deployment and the meaning of the following configurations
      client_url: 'http://127.0.0.1:50000/' # CosyVoice gradio demo webui url
      mode_checkbox_group: '预训练音色'
      sft_dropdown: '中文女'
      prompt_text: ''
      prompt_wav_upload_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav'
      prompt_wav_record_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav'
      instruct_text: ''
      seed: 0
      api_name: '/generate_audio'

    cosyvoice2_tts: # Cosy Voice TTS connects to the gradio webui
      # Check their documentation for deployment and the meaning of the following configurations
      client_url: 'http://127.0.0.1:50000/' # CosyVoice gradio demo webui url
      mode_checkbox_group: '3s极速复刻' 
      sft_dropdown: '' 
      prompt_text: '' 
      prompt_wav_upload_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav' 
      prompt_wav_record_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav' 
      instruct_text: '' 
      stream: False 
      seed: 0 
      speed: 1.0 
      api_name: '/generate_audio' 

    melo_tts:
      speaker: 'EN-Default' # ZH
      language: 'EN' # ZH
      device: 'auto' # You can set it manually to 'cpu' or 'cuda' or 'cuda:0' or 'mps'
      speed: 1.0

    x_tts:
      api_url: 'http://127.0.0.1:8020/tts_to_audio'
      speaker_wav: 'female'
      language: 'en'

    gpt_sovits_tts:
      # put ref audio to root path of GPT-Sovits, or set the path here
      api_url: 'http://127.0.0.1:9880/tts'
      text_lang: 'zh'
      ref_audio_path: ''
      prompt_lang: 'zh'
      prompt_text: ''
      text_split_method: 'cut5'
      batch_size: '1'
      media_type: 'wav'
      streaming_mode: 'false'

    fish_api_tts:
      # The API key for the Fish TTS API.
      api_key: ''
      # The reference ID for the voice to be used. Get it on the [Fish Audio website](https://fish.audio/).
      reference_id: ''
      # Either 'normal' or 'balanced'. balance is faster but lower quality.
      latency: 'balanced'
      base_url: 'https://api.fish.audio'

    coqui_tts:
      # Name of the TTS model to use. If empty, will use default model
      # do 'tts --list_models' to list supported models for coqui-tts
      # Some examples:
      # - 'tts_models/en/ljspeech/tacotron2-DDC' (single speaker)
      # - 'tts_models/zh-CN/baker/tacotron2-DDC-GST' (single speaker for chinese)
      # - 'tts_models/multilingual/multi-dataset/your_tts' (multi-speaker)
      # - 'tts_models/multilingual/multi-dataset/xtts_v2' (multi-speaker)
      model_name: 'tts_models/en/ljspeech/tacotron2-DDC'
      speaker_wav: ''
      language: 'en'
      device: ''

    siliconflow_tts:
      api_url: "https://api.siliconflow.cn/v1/audio/speech"
      api_key: "your key"
      default_model: "FunAudioLLM/CosyVoice2-0.5B"
      default_voice: "speech:Dreamflowers:5bdstvc39i:xkqldnpasqmoqbakubom your voice name"  # Default voice configuration in the format: "speech:MODEL_NAME:VOICE_ID:your voice name"
      sample_rate: 32000  # Control the output sample rate. The default values and differ for different video output types, as follows: opus: Supports 48000 Hz. wav, pcm: Supports 8000, 16000, 24000, 32000, 44100 Hz, with a default of 44100 Hz. mp3: Supports 32000, 44100 Hz, with a default of 44100 Hz.
      response_format: "mp3" # The format to audio out. Supported formats are mp3, opus, wav, pcm
      stream: true
      speed: 1
      gain: 0

    # pip install sherpa-onnx
    # documentation: https://k2-fsa.github.io/sherpa/onnx/index.html
    # TTS models download: https://github.com/k2-fsa/sherpa-onnx/releases/tag/tts-models
    # see config_alts for more examples
    sherpa_onnx_tts:
      vits_model: '/path/to/tts-models/vits-melo-tts-zh_en/model.onnx' # Path to VITS model file
      vits_lexicon: '/path/to/tts-models/vits-melo-tts-zh_en/lexicon.txt' # Path to lexicon file (optional)
      vits_tokens: '/path/to/tts-models/vits-melo-tts-zh_en/tokens.txt' # Path to tokens file
      vits_data_dir: '' # '/path/to/tts-models/vits-piper-en_GB-cori-high/espeak-ng-data'  # Path to espeak-ng data (optional)
      vits_dict_dir: '/path/to/tts-models/vits-melo-tts-zh_en/dict' # Path to Jieba dict (optional, for Chinese)
      tts_rule_fsts: '/path/to/tts-models/vits-melo-tts-zh_en/number.fst,/path/to/tts-models/vits-melo-tts-zh_en/phone.fst,/path/to/tts-models/vits-melo-tts-zh_en/date.fst,/path/to/tts-models/vits-melo-tts-zh_en/new_heteronym.fst' # Path to rule FSTs file (optional)
      max_num_sentences: 2 # Max sentences per batch (or -1 for all)
      sid: 1 # Speaker ID (for multi-speaker models)
      provider: 'cpu' # Use 'cpu', 'cuda' (GPU), or 'coreml' (Apple)
      num_threads: 1 # Number of computation threads
      speed: 1.0 # Speech speed (1.0 is normal)
      debug: false # Enable debug mode (True/False)
    
    spark_tts:
      api_url: 'http://127.0.0.1:6006/' # API URL. Uses Gradio's built-in front-end API. Repository: https://github.com/SparkAudio/Spark-TTS
      api_name:  "voice_clone" # Endpoint name. Options: voice_clone, voice_creation
      prompt_wav_upload: "https://uploadstatic.mihoyo.com/ys-obc/2022/11/02/16576950/4d9feb71760c5e8eb5f6c700df12fa0c_6824265537002152805.mp3" # Reference audio URL. Provide if api_name equals "voice_clone"
      gender:  "female" # Voice type (gender). Provide if api_name equals "voice_creation"
      pitch:  3 # Pitch shift (in semitones) default 3,range 1-5. Valid only if api_name equals "voice_creation"
      speed:  3 # Speed of the voice (in percent) default 3,range 1-5. Valid only if api_name equals "voice_creation"

    openai_tts: # Configuration for OpenAI-compatible TTS endpoints
      # These settings override the defaults in the openai_tts.py file if provided
      model: 'kokoro' # Model name expected by the server (e.g., 'tts-1', 'kokoro')
      voice: 'af_sky+af_bella' # Voice name(s) expected by the server (e.g., 'alloy', 'af_sky+af_bella')
      api_key: 'not-needed' # API key if required by the server
      base_url: 'http://localhost:8880/v1' # Base URL of the TTS server
      file_extension: 'mp3' # Audio file format ('mp3' or 'wav')

    # For more details, see: https://platform.minimaxi.com/document/Announcement
    minimax_tts:
      group_id: '' # Your minimax group_id
      api_key: '' # Your minimax api_key
      # Supported models: 'speech-02-hd', 'speech-02-turbo' (recommended: 'speech-02-turbo')
      model: 'speech-02-turbo' # minimax model name
      voice_id: 'female-shaonv' # minimax voice id, default is 'female-shaonv'
      # Custom pronunciation dictionary, default empty.
      # Example: '{"tone": ["测试/(ce4)(shi4)", "危险/dangerous"]}'
      pronunciation_dict: ''

    elevenlabs_tts:
      api_key: ''
      voice_id: '' # Voice ID from ElevenLabs
      model_id: 'eleven_multilingual_v2' # Model ID (e.g., eleven_multilingual_v2)
      output_format: 'mp3_44100_128' # Output audio format (e.g., mp3_44100_128)
      stability: 0.5 # Voice stability (0.0 to 1.0)
      similarity_boost: 0.5 # Voice similarity boost (0.0 to 1.0)
      style: 0.0 # Voice style exaggeration (0.0 to 1.0)
      use_speaker_boost: true # Enable speaker boost for better quality
    
    cartesia_tts:
      api_key: ''
      voice_id: '' # Voice ID from Cartesia
      model_id: 'sonic-3' # Model ID (e.g., sonic-3)
      output_format: 'wav' # Output audio format (e.g., wav)
      language: 'en' # Output language of voice (e.g., en)
      emotion: 'neutral' # Emotional guidance
      volume: 1.0 # Voice volume (0.5 to 2.0)
      speed: 1.0 # Voice speed (0.6 to 1.5)

  # =================== Voice Activity Detection ===================
  vad_config:
    vad_model: null

    silero_vad:
      orig_sr: 16000 # Original Audio Sample Rate
      target_sr: 16000 # Target Audio Sample Rate
      prob_threshold: 0.4 # Probability Threshold for VAD
      db_threshold: 60 # Decibel Threshold for VAD
      required_hits: 3 # Number of consecutive hits required to consider speech
      required_misses: 24 # Number of consecutive misses required to consider silence
      smoothing_window: 5 # Smoothing window size for VAD

  tts_preprocessor_config:
    # settings regarding preprocessing for text that goes into TTS

    remove_special_char: True # remove special characters like emoji from audio generation
    ignore_brackets: True # ignore everything inside brackets
    ignore_parentheses: True # ignore everything inside parentheses
    ignore_asterisks: True # ignore everything wrapped inside asterisks
    ignore_angle_brackets: True # ignore everything wrapped inside <text>

    translator_config:
      # Like... you speak and read the subtitles in English, and the TTS speaks Japanese or that kind of things
      translate_audio: False # Warning: you need to deploy DeeplX to use this. Otherwise it's going to crash
      translate_provider: 'deeplx' # deeplx or tencent

      deeplx:
        deeplx_target_lang: 'JA'
        deeplx_api_endpoint: 'http://localhost:1188/v2/translate'
      
      #  Tencent Text Translation  5 million characters per month  Remember to turn off post-payment, need to manually go to Machine Translation Console > System Settings to disable
      #   https://cloud.tencent.com/document/product/551/35017
      #   https://console.cloud.tencent.com/cam/capi
      tencent:
        secret_id: ''
        secret_key: ''
        region: 'ap-guangzhou'
        source_lang: 'zh'
        target_lang: 'ja'

# Live Streaming Integration
live_config:
  bilibili_live:
    # List of BiliBili live room IDs to monitor
    room_ids: [1991478060]
    # SESSDATA cookie value (optional, for authenticated requests)
    sessdata: ""
